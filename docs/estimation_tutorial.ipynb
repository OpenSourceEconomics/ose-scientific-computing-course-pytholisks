{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimation Tutorial\n",
    "\n",
    "In this section, we dive into the topic of model estimation using **pydsge**. Note that for this tutorial we will assume a folder set-up of the form\n",
    "\n",
    "```\n",
    "analysis/\n",
    "├── README.md\n",
    "├── src/ \n",
    "│   ├── estimation.py or .ipynb \n",
    "│   └── model.yaml   \n",
    "├── data/\n",
    "│   └── example_data\n",
    "└── output/\n",
    "```\n",
    "This is because the estimation creates (intermediate) output results, which we will want to store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just for the tutorial: Setting up example structure\n",
    "import tempfile\n",
    "import os\n",
    "import shutil # For clean-up of temporary directory\n",
    "from pathlib import Path # For Windows/Unix compatibility\n",
    "\n",
    "# Temporary output folder\n",
    "output_path = Path(tempfile.gettempdir(), 'output')\n",
    "os.makedirs(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing and loading the model\n",
    "\n",
    "Let us first load the relevant packages. Besides the DSGE class we already know from [*getting started*](https://pydsge.readthedocs.io/en/latest/getting_started.html), we also want to import the `emcee` package. This will allow us to later specify the desired updating algorithms for sampling from the posterior distribution - we explain this in more detail below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import emcee # For specifying updating moves\n",
    "\n",
    "from pydsge import DSGE, example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we continue to use the example provided in `pydsge`. Like before, we specify the file paths of the model and the data. Please feel free to check-out both files, but from the previous tutorial you might remember that we're dealing with a five equations New Keynesian model and US quarterly data from 1995 to 2018. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml_file, data_file = example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We again parse the model and load-in the data. What is important is that we also specify a location where the (intermediate) output is stored. Here we assign the output folder, as discussed at the beginning. Note also that we can name the model and write a short description, which is very useful when working with several models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the model\n",
    "mod = DSGE.read(yaml_file)  \n",
    "\n",
    "# Give it a name\n",
    "mod.name = 'Rank_tutorial'\n",
    "mod.description = 'RANK, estimation tutorial'\n",
    "\n",
    "# Storage location for output\n",
    "mod.path = output_path\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(data_file, parse_dates=['date'], index_col=['date'])\n",
    "df.index.freq = 'Q' # let pandas know that this is quartely data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that since the Great Recession, the Federal Funds Rate has been below the ZLB. That is why, like in [*getting started*](https://pydsge.readthedocs.io/en/latest/getting_started.html), we adjust the observed interest rate, so that the data is within reach of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GDP</th>\n",
       "      <th>Infl</th>\n",
       "      <th>FFR</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1998-03-31</th>\n",
       "      <td>0.77834</td>\n",
       "      <td>0.14386</td>\n",
       "      <td>1.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998-06-30</th>\n",
       "      <td>0.69635</td>\n",
       "      <td>0.22873</td>\n",
       "      <td>1.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998-09-30</th>\n",
       "      <td>1.03077</td>\n",
       "      <td>0.36109</td>\n",
       "      <td>1.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998-12-31</th>\n",
       "      <td>1.37921</td>\n",
       "      <td>0.26145</td>\n",
       "      <td>1.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999-03-31</th>\n",
       "      <td>0.54307</td>\n",
       "      <td>0.37393</td>\n",
       "      <td>1.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-03-31</th>\n",
       "      <td>0.41475</td>\n",
       "      <td>0.49969</td>\n",
       "      <td>0.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-06-30</th>\n",
       "      <td>0.54594</td>\n",
       "      <td>0.25245</td>\n",
       "      <td>0.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-30</th>\n",
       "      <td>0.54391</td>\n",
       "      <td>0.51972</td>\n",
       "      <td>0.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-31</th>\n",
       "      <td>0.48458</td>\n",
       "      <td>0.57830</td>\n",
       "      <td>0.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-31</th>\n",
       "      <td>0.15170</td>\n",
       "      <td>0.48097</td>\n",
       "      <td>0.36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>81 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                GDP     Infl   FFR\n",
       "date                              \n",
       "1998-03-31  0.77834  0.14386  1.38\n",
       "1998-06-30  0.69635  0.22873  1.38\n",
       "1998-09-30  1.03077  0.36109  1.38\n",
       "1998-12-31  1.37921  0.26145  1.22\n",
       "1999-03-31  0.54307  0.37393  1.18\n",
       "...             ...      ...   ...\n",
       "2017-03-31  0.41475  0.49969  0.18\n",
       "2017-06-30  0.54594  0.25245  0.24\n",
       "2017-09-30  0.54391  0.51972  0.29\n",
       "2017-12-31  0.48458  0.57830  0.30\n",
       "2018-03-31  0.15170  0.48097  0.36\n",
       "\n",
       "[81 rows x 3 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# adjust elb\n",
    "zlb = mod.get_par('elb_level')\n",
    "rate = df['FFR']\n",
    "df['FFR'] = np.maximum(rate,zlb)\n",
    "\n",
    "mod.load_data(df, start='1998Q1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After importing the packages and loading the data, we still need to tell pydsge how to carry out the estimation of our model. The \"prep_estim\" method can be used to accomplish this. It can be called without any arguments and sets-up a non-linear model by default. However, to showcase some of this functionality, we decide to specify several arguments here.\n",
    "\n",
    "To perform the estimation, `pydsge` uses a Transposed-Ensemble Kalman Filter (TEnKF). For general information on its implementation, see the [EconSieve documentation](https://econsieve.readthedocs.io/en/latest/) , and for more details on running the filter in `pydsge` check-out the [*getting started tutorial*](https://pydsge.readthedocs.io/en/latest/getting_started.html). Again,  the default filter is non-linear, but we can opt for a linear one by setting the argument `Linear` to `True`. To choose a custom number of ensemble members for the TEnKF, set `N` to a particular number (default is 300). We can also set a specific `seed`, the default seed is `0`. To get additional information on the estimation process, we can set  `verbose` to `True`. Conveniently, this information includes an overview of the parameters’ distribution, their means and standard deviations. Moreover, if we already specified the covariance matrix of the measurement errors or want to reuse a previous result, we can load it into the `prep_estim` method by setting `Load.R` to `True`. Finally, we can turn parallelization on or off with the debug argument, which can be helpful in case any issues should arise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mod.prep_estim(N=350, seed=0, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After finishing our set-up, the only thing left to prepare is to filter our observed FFR for hidden states. We can simply identify the variable through `index` and, given the present context, set the measurement error to a very small value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mod.filter.R = mod.create_obs_cov(1e-1)\n",
    "ind = mod.observables.index('FFR')\n",
    "mod.filter.R[ind,ind] /= 1e1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the we have all the variables and defined the type of estimation to perform, we can turn to estimating the model. To be able to deal with very high-dimensional models, `pdygse` uses *Markov Chain Monte Carlo* (MCMC) Integration to sample from the posterior distribution. For further information on MCMC, please refer to the `emcee` [website](https://emcee.readthedocs.io/en/stable/) and the additional resources provided there. We recommend running a **Tempered Ensemble MCMC** first, by using the `tmcmc` method. Doing this is particularly valuable for high-dimensional problems, since defining the initial states of the walkers in the parameterspace in this way is a powerful tool to improve sampling. However, due to its computational efficiency, we also use it for small models such as the one we are dealing with here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our ensemble sampling, we can specify a variety of options. Note, `tmcmc` always requires the specification of the first four arguments, which are the i) number of steps, ii) number of walks, iii) number of temperatures, and iv) a temperature target! Here we do not want to set a target and, in turn, set `fmax = None`. Moreover, we have the option to set different \"moves\", i.e. coordinate updating algorithms for the walkers. As a wrapper for a lot of `emcee` functionality,  `tmcmc` can work with many different \"moves\" - for a list and implementation details please consult the `emcee` documentation. For using them here, specify them as a list of tuples, containing the type of move and its \"weight\". If no move is specified, \"StretchMove\" is used. For seed setting of the log probability, the user can choose between three options, here we use the seed specified in `prep_estim`. Finally, the states are saved in the `p0` object as a numpy array in order to later pass them to our main sampling process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmax = None\n",
    "\n",
    "moves = [(emcee.moves.DEMove(), 0.8), \n",
    "         (emcee.moves.DESnookerMove(), 0.2),]\n",
    "\n",
    "p0 = mod.tmcmc(200, 200, 0, fmax, moves=moves, update_freq=100, lprob_seed='set')\n",
    "mod.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the output provides us with various important details. In particular, we learn that `mod.save()` saved the meta data of our model in the directory which we specified earlier in `mod.path`. This information is stored as an `.npz` file so that it is avialable even in the event of a crash and can be loaded anytime using `numpy.load()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now use the initial states derived above to conduct our full Bayesian estimation. Still, initial states do not have to be specified and, unless `mcmc` can identify previous runs or estimations, the initial values of the \"prior\" section in the `*.yaml` are used. The default number of sampling steps is 3000, so it makes sense to allow this to run in parallel. Again, if you want to avoid this, simply set `debug` to `True`. With `tune` we can determine the size of the Markov Chain we wish to retain. It is important to not confuse this with the updating frequency, which only affects the number of summary statements `pydsge`reports during the estimation. Note that, like in the `tmcmc`, we choose to continue using the seed specified earlier. Lastly, the option `append` lets us store all intermediate results. We pickle and store the meta information of this object in the path specified earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod.mcmc(p0,\n",
    "         moves=moves,\n",
    "         nsteps=3000,\n",
    "         tune=500,\n",
    "         update_freq=500,\n",
    "         lprob_seed='set',\n",
    "         append=True,\n",
    "         debug=True,\n",
    "         )\n",
    "mod.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But, so where are our estimates? Remember that, so far, we have only drawn samples from our posterior distribution. Our converged (burnt-in) MCMC samples are currently stored in the `rank_test_sampler.h5` file created by `mcmc`. To get our parameter estimates, we now still need to draw a sample form the MCMC object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pars = mod.get_par('posterior', nsamples=250, full=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's have a look at the estimated shocks. We can do this by using `extract()` which gives us the smoothed shocks. This method takes a variety of arguments, all of which have sensible default values. For example, here we specify the number of parameter draws in each verification sample to 1. Note that this method also takes an optional seed argument, but we here continue to use the default seed 0.  It is important to emphasise that `pysdge` seeks to separate the model's set-up (meta) data and its results. To store the Markov Chains, shocks and parameter estimates we use the `save_rdict()` method, below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsdf = mod.extract(pars, nsamples=1)\n",
    "mod.save_rdict(epsdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we take a closer look at the MCMC estimation results. In particular, `mcmc_summary()` summaries the convergence behaviour of our draws from the posterior distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod.mcmc_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just for the tutorial: Cleaning the temporary directory\n",
    "shutil.rmtree(output_path)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d226595e2f076559e618d0a9d30d224ac27d056d5cb4864945e1f27051c61083"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('stud_proj': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
